\section{Lista 1: Estatísticas de Segunda Ordem} % <-----------------------------------------------------------------------------


\subsection{Média e Autocorrelação} % <-----------------------------------------------------------------------------
Para obter a média, basta a expressão de acordo com o operador esperança $\mathbb{E}\{ \cdot \}$, dado que as variáveis aleatórias tem mesma média, resume-se a expressão:
				
\begin{align*}
    \mathbb{E}\{ x(n)\} &= \mathbb{E}\{ v(n) + 3v(n-1)\} \\
    &= \mu + 3\mu \\
    &= 4\mu
\end{align*}

Já a variância, é obtida aplicando o mesmo operador, "abrindo" o termo ao quadrado, reorganizando em função do termo $\sigma^{2}$ e sabendo que $v(n)$ e $v(n-1)$ são descorrelacionadas:

\begin{align*}
    \mathbb{E}\{[(x(n) - \mu_{X})]^2\} &= \mathbb{E}\{[v(n) - 3v(n-1) - \mu_{X}]^{2}\} \\
    &= \mathbb{E}\{[v(n) - 3v(n-1) - 4\mu]^{2}\} \\
    &= \mathbb{E}\{[v(n) - \mu + 3v(n-1) -3\mu]^{2}\} \\
    &= \sigma^{2} + 9\sigma^{2} + \mathbb{E}\{6[v(n) - \mu][v(n-1) - \mu]\} \\
    &= 10\sigma^{2}
\end{align*}


Para afirmar que o processo apresentado é estacionário em sentido amplo, abrevidado em inglês para \textbf{WSS}, as estatísticas de primeira e de segunda ordem devem ser independentes ao deslocamento no tempo. Isto pode ser observado, assumindo novamente que $x(n)$ e $x(n+\tau)$, via função de correlação, dada por:

\begin{align*} 
    \mathbb{E}\{x(n)x(n+\tau)\} &= \mathbb{E}\{[v(n) + 3v(n-1)][v(n+\tau) + 3v(n-1 +\tau)]\}, \\
    &= \mathbb{E}\{ v(n)v(n+\tau) + 3v(n)v(n-1 +\tau) + 3v(n-1)v(n+\tau) + 9v(n-1)v(n-1+\tau) \} \\
    &= \mathbb{E}\{\mu^{2} + 3\mu^{2} + 3\mu^{2} + 9\mu^{2}\} \\
    &= 16\mu^{2}
\end{align*}

Visto que os pré-requisitos são cumpridos, pode-se concluir que o processo é de fato WSS. Entretanto, para afirmar algo além disso é necessário conhecer os movimentos de ordem superior do caso estudado.


\subsection{Processos Estacionários} % <-----------------------------------------------------------------------------
    
\textbf{Funções de autocorrelação de $x$ e de $y$}

Primeiramente, é conveniente definir o processo de ruído branco, visto que este possui propriedades bastante conveniente para a solução do problema. O processo desta natureza tem média nula e tem todas as suas amostras independentes entre si. Isto permite que seja obtida a média de novos processos resultantes da mistura linear desses ruídos.

Para $x(n)$, obtém-se a média dado:
\begin{align*}
    \mathbb{E}\{x(n)\} &= \mathbb{E}\{v_{1}(n) + 3v_{2}(n-1)\} \\
    & = \mu_{1} + 3\mu_{2} \\
    & = 0 
\end{align*}

Enquanto para a variânciam, tem-se que (semelhante ao exercício 1.1):

\begin{align*}
    \mathbb{E}\{[x(n) - \mu]^{2}\} &= \mathbb{E}\{[x(n) - 0]^{2}\}\\
    &= \mathbb{E}\{[v_{1}(n) + 3v_{2}(n-1)]^{2}\} \\
    &= \mathbb{E}\{ [v^{2}_{1}(n)] + 6[v_{1}(n)v_{2}(n-1)] + 9[v^{2}_{2}(n-1)]\} \\
    &= 1 \sigma^{2}_{1} + 9 \sigma^{2}_{2} \\
    &= 5
\end{align*}

O mesmo procedimento é aplicado para $y(n)$:

\begin{align*}
    \mathbb{E}\{y(n)\} &= \mathbb{E}\{v_{2}(n+1) + 3v_{1}(n-1)\} \\
    &= \mu_{2} + 3\mu_{1} \\
    &= 0
\end{align*}

\begin{align*}
    \mathbb{E}\{[y(n) - \mu]^{2}\} &= \mathbb{E}\{[y(n) - 0]^{2}\} \\
    &= \mathbb{E}\{[v_{2}(n+1) + 3v_{1}(n-1)]^{2}\} \\
    &= \mathbb{E}\{ [v^{2}_{2}(n)] + 6[v_{2}(n+1)v_{1}(n-1)] + 9[v^{1}_{2}(n-1)]\} \\
    &= 9 \sigma^{2}_{1} + 1 \sigma^{2}_{2} \\
    &= 5
\end{align*}

Para função de autocorrelação de $x(n)$, sabe-se que as amostras são descorrelacionadas e o processo é de média nula, então o produto de diversos termos igual a zero também é zero. Temos que:

\begin{align*}
    r_{x}(\tau) &= \mathbb{E}\{x(n)x(n + \tau)\} = \mathbb{E}\{[v_{1}(n) + 3v_{2}(n-1)][v_{1}(n + \tau) + 3v_{2}(n-1 + \tau)]\}, \\
    &= \mathbb{E}\{v_{1}(n)v_{1}(n + \tau) + 3v_{1}(n)v_{2}(n-1 + \tau) + 3v_{2}(n-1)v_{1}(n + \tau) + 9v_{2}(n-1)v_{2}(n-1 + \tau) \}\\
    &= \vdots \quad \text{(Mesmo passo a passo do problema 1.1)} \\
    &= 0
\end{align*}

Para $y(n)$, o processo é o mesmo, consquentemente  $r_{y}(\tau) = 0$.

Finalmente, observa-se que estatísticas de primeira e de segunda ordem são independentes do tempo para ambos, i.e, os dois processos são \textbf{WSS}.


\textbf{Função de correlação cruzada}

Para obter a função de correlação cruzada, basta aplicar as premissas utilizadas anteriormente: I) Processo de ruído branco é descorrelacionado; II) Média nula.
\begin{align*}
    r_{x,y}(n_{1},n_{0}) &= \mathbb{E}\{[x(n_{1})y^{*}(n_{0})]\} \\
    & = \mathbb{E}\{[v_{1}(n_{1}) + 3v_{2}(n_{1}-1)][v_{2}(n_{0}+1) + 3v_{1}(n_{0}-1)]^{*}\}, \\
    &= \mathbb{E}\{v_{1}(n_{1})v^{*}_{2}(n_{0}+1) + 3v_{1}(n_{1})v^{*}_{1}(n_{0}-1) + 3v_{2}(n_{1}-1)v^{*}_{2}(n_{0}+1) + 9v_{2}(n_{1}-1)v^{*}_{1}(n_{0}-1)\} \\
    &= 0
\end{align*}

Esta função também é igual a zero, $r_{x,y}(n_{1},n_{0}) = 0$. Isto implica que os processos são conjuntamente estacionários, pois há independência do tempo da função, e por partir de processos de ruído branco, processos WSS individualmente, isto sustenta os desenvolvimento acima.


\subsection{Matriz de Autocorrelação} % <-----------------------------------------------------------------------------

\textbf{Um vetor aleatório bidimensional}

Para garantir a existência da matriz de correlação, deve-se corresponder as seguintes premissas: I) $\mathbf{R_{x}} = \mathbf{R^{H}_{x}}$; II) $\mathbf{a^{H}} \mathbf{R_{xa}} \geq 0$; III) $\mathbf{Ax} = \lambda \mathbf{x}, \forall \lambda \geq 0 | \mathbf{x} \in \mathbb{R}$.

Assumindo um vetor aleatório bidimensional: $\mathbf{X} = (x_{1},x_{2})$, a existência de $\mathbf{R}$ e sua hermitiana, $\mathbf{R}^{H}$. 

Os elementos da contra-diagonal da matriz hermitiana deve obdece para são equivalência simétrica:
I) $\mathbb{E}\{[x_{1}x^{*}_{2}]\} = \mathbb{E}\{[x_{2}x^{*}_{1}]\}$ e II) $\mathbb{E}\{[x_{2}x^{*}_{1}]\} = \mathbb{E}\{[x_{1}x^{*}_{2}]\}$

Além disso, a vantagem de $\mathbb{E}\{\cdot\}$ ser um operador linear, garante que os resultados são de fato iguais, independente da ordem dos vetores.

Já a limitação dos autovalores está diretamente ao determinante da matriz, sendo esse maior que zero, o critério imposto é atingido, i.e, para o caso $2 \times 2$, o produto dos elementos da diagonal principal é maior que o produto dos elementos da contra-diagonal.\\

\textbf{Processo estocástico estacionário escalar}

Em processo semelhante ao exemplo anterior, assume-se um processo estocástico estacionário escalar do tipo $\mathbf{X}_{(t)} = x(t)$ e sua versão atrasada $\mathbf{X}_{(t + \tau)} = x(t + \tau)$. Dado a matriz $\mathbf{R}$ e sua hermitiana, $\mathbf{R}^{H}$.

Os elementos da contra-diagonal da matriz hermitiana deve obdece para são equivalência simétrica:
I) $\mathbb{E}\{[x(t)x^{*}(t + \tau)]\} = \mathbb{E}\{[x(t + \tau)x^{*}(t)]\}$ e II) $\mathbb{E}\{[x(t + \tau)x^{*}(t)]\} = \mathbb{E}\{[x(t)x^{*}(t + \tau)]\}$.

Novamente, a vantagem do operador linear é conveniente para que independente da ordem, e igualdade na contra-diagonal, i.e, simetria.

Para os autovalores, o produto dos elementos da diagonal principal é maior que o produto dos elementos da contra-diagonal, i.e:
\begin{align*}
    \mathbb{E}\{[x^{2}(t)]\} \mathbb{E}\{[x^{2}(t + \tau)]\} > \mathbb{E}\{[x(t)x^{*}(t + \tau)]]\}  \mathbb{E}\{[x(t + \tau)]x^{*}(t)]\}
\end{align*}


\subsection{Matriz Definida Positiva} % <-----------------------------------------------------------------------------

Assumindo a expressão que define matriz de autocorrelação e que existe sua inversa bem, 

\begin{align*}
    \mathbb{E}\{\mathbf{x} \mathbf{x}^{\text{H}}\} &= \mathbf{R}_{x} \\
    \mathbb{E}\{\mathbf{x} \mathbf{x}^{\text{H}}\}\mathbf{R}^{-1}_{x} &= \mathbf{R}_{x}\mathbf{R}^{-1}_{x} \\
\end{align*}

A inversa pode adentrar o operador, enquanto do lado direito obtém-se uma matrix identidade
\begin{align*}
    \mathbb{E}\{\mathbf{x} \mathbf{x}^{\text{H}}\mathbf{R}^{-1}_{x}\} = \mathbf{I}_{N \times N}
\end{align*}

Isto permite aplicar o traço da matriz e por meio da propriedade de permutação cíclica do operador, tem-se que:
\begin{align*}
    \text{Trace}\{\mathbb{E}\{\mathbf{x} \mathbf{x}^{\text{H}}\mathbf{R}^{-1}_{x}\}\} &= \text{Tr}\{\mathbf{I}_{N \times N}\} 
\end{align*}

Observa-se que o traço da matriz identidade $\mathbf{I}_{N \times N}$ é justamente a soma dos elementos da diagonal, $N$.

\begin{align*}
    \text{Trace}\{\mathbb{E}\{\mathbf{x}^{\text{H}}\mathbf{R}^{-1}_{x} \mathbf{x}\}\} &= \sum^{N} 1 \\
    &= N
\end{align*}


\subsection{Covariância e correlação} % <-----------------------------------------------------------------------------

\textbf{Expressão 1}
Dado que a matriz de Covariância pode ser obtida por:
\begin{align*}
    C_{X} &= \mathbb{E}\{[(x - \mu)(x - \mu)^{H}]\} \\
    &= \mathbb{E}\{xx^{H}\} -\mathbb{E}\{x\mu^{H}\} - \mathbb{E}\{\mu x^{H}\} + \mathbb{E}\{\mu \mu^{H}\}
\end{align*}

Considerando que a matriz de correlação pode ser escrita como demonstrado no exercício 1.4:

\begin{align*}
    C_{X} &= R_{X} - \mu^{H}\mathbb{E}\{[x]\} - \mu \mathbb{E}\{[x^{H}]\} + \mu \mu^{H}, \\
    &= R_{X} - \mu \mu^{H} - \mu \mu^{H} + \mu \mu^{H}, \\
    &= R_{X} - \mu \mu^{H}, \\
\end{align*}    

Por fim, obtém-se que:
\begin{align*}
    &R_{X} = C_{X} + \mu \mu^{H}.
\end{align*}

\textbf{Expressão 2}
As expressões de correlação cruzada são obtidos de forma análoga, tal que:


\begin{align}
    \mathbf{C_{xy}} &= \mathbb{E}\{[x - \mu_{x}][y - \mu_{y}]\}, \\
    &= \mathbb{E}\{[xy]\} - \mathbb{E}\{[x\mu_{y}]\} - \mathbb{E}\{[\mu_{x} y]\} + \mathbb{E}\{[\mu_{x} \mu_{y}]\} \\
    &= \mathbb{E}\{[xy]\} - \mu_{y} \mu_{x} - \mu_{x} \mu_{y} + \mu_{x} \mu_{y} \\
    &= \mathbb{E}\{[xy]\} + \mu_{x} \mu_{y}, \\
    &= \mu_{x} \mu_{y} \\
\end{align}

De forma análoga, obtém que $\mathbf{C_{yx}} = - \mu_{x} \mu_{y}$, consquentemente
\begin{align*}
    \mathbf{C_{xy}} + \mathbf{C_{yx}} &= \mu_{x} \mu_{y} - \mu_{x} \mu_{y} \\
    &= 0
\end{align*}


\subsection{Função de autocorrelação} % <-----------------------------------------------------------------------------
\textbf{Função do Processo}
Como nos problemas anteriores, utiliza-se como premissa que os processos são descorrelacionados e tem média nula. A função é dada por $r_{x} = \mathbb{E}\{x(n) x^{*}(n)\}$, tal que

\begin{align*}
    r_{x} &= \mathbb{E}\{[v_{1}(n) + 2v_{1}(n+1) + 3v_{2}(n-1)] [v_{1}(n) + 2v_{1}(n+1) + 3v_{2}(n-1)]^{*}(n)\} \\
    &= r_{v}(n,n) + 2r_{v}(n,n+1) + 2r_{v}(n+1,n) + 4r_{v}(n+1,n+1) + 9r_{v}(n-1,n-1)
\end{align*}

Observa-se que apenas termos onde a função degrau está presente permanecem, enquanto o restantes podem ser cancelados, de modo que:
\begin{align*}
    r_{x} &= 2r_{v}(n,n+1) + 2r_{v}(n+1,n)\\
    &= \delta(n - n - 1) + \delta(n + 1 - n)
\end{align*}

Isto pode ser reorganizado, sendo $\tau = n_{1} - n_{2}$, de modo que:

\begin{align*}
    r_{x}(n_{1}, n_{2}) = \delta(\tau) + \delta(-\tau)
\end{align*}

Há apenas um deslocamento temporal $(\tau)$ atrelado à correlação, logo o processo é WSS.


\textbf{Matriz de Correlação}

Utilizando as relações obtidas anteriormente, é possível observar que os únicos elementos não nulos pertencem à diagonal, onde $n_{1}= n_{2}$, acarretando $\delta(0) + \delta(0) = 2$.

Considerando 8 amostras consecutivas, a matriz de correlação é dada por:
\begin{align*}
    \mathbf{R}_{\mathbf{x}} = 2 \times I_{8 \times 8}
\end{align*}

Isto é, uma matriz $8 \times 8$, onde apenas a diagonal é não nula, preenchida por 2.